# Welcome to VAST Organization üëã

We are VAST, the team behind [Tripo](https://www.tripo3d.ai) - a cutting-edge 3D generation platform. Our mission is to revolutionize the way 3D content is created through advanced AI technology.

## üåü Our Products

### Tripo Ecosystem

#### Core Products
- **[TripoSG](https://github.com/VAST-AI-Research/TripoSG)**: High-Fidelity 3D Shape Generation using Large-Scale Rectified Flow Models
- **[TripoSF](https://github.com/VAST-AI-Research/TripoSF)**: SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Generation
- **[TripoSR](https://github.com/VAST-AI-Research/TripoSR)**: Fast 3D Object Generation from a Single Image

#### Tools & Integration
- **[tripo-mcp](https://github.com/VAST-AI-Research/tripo-mcp)**: Official MCP server for Tripo
- **[tripo-python-sdk](https://github.com/VAST-AI-Research/tripo-python-sdk)**: Official Python SDK for Tripo
- **[ComfyUI-Tripo](https://github.com/VAST-AI-Research/ComfyUI-Tripo)**: Official custom nodes for using Tripo in ComfyUI

#### Research Projects
- **[MIDI-3D](https://github.com/VAST-AI-Research/MIDI-3D)**: [CVPR 2025] Multi-Instance Diffusion for Single Image to 3D Scene Generation
- **[Deformable-Radial-Kernel-Splatting](https://github.com/VAST-AI-Research/Deformable-Radial-Kernel-Splatting)**: [CVPR 2025] Advanced 3D generation techniques
- **[TriplaneGaussian](https://github.com/VAST-AI-Research/TriplaneGaussian)**: [CVPR 2024] A new hybrid representation for single-view 3D generation
- **[Text-to-3D with Classifier Score Distillation](https://xinyu-andy.github.io/Classifier-Score-Distillation/)**: [ICLR 2024] [[Code]](https://github.com/CVMI-Lab/Classifier-Score-Distillation)
- **[EpiDiff](https://huanngzh.github.io/EpiDiff/)**: [CVPR 2024] Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion [[Code]](https://github.com/huanngzh/EpiDiff)
- **[Wonder3D](https://www.xxlong.site/Wonder3D/)**: [CVPR 2024] Single Image to 3D using Cross-Domain Diffusion [[Code]](https://github.com/xxlong0/Wonder3D)
- **[DreamComposer](https://yhyang-myron.github.io/DreamComposer/)**: [CVPR 2024] Controllable 3D Object Generation via Multi-View Conditions [[Code]](https://github.com/yhyang-myron/DreamComposer)
- **[SC-GS](https://yihua7.github.io/SC-GS-web/)**: [CVPR 2024] Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes [[Code]](https://github.com/yihua7/SC-GS)
- **[CharacterGen](https://charactergen.github.io/)**: [SIGGRAPH 2024] Efficient 3D Character Generation from Single Images [[Code]](https://github.com/zjp-shadow/CharacterGen)
- **[HiFi-123](https://drexubery.github.io/HiFi-123/)**: [ECCV 2024] Towards High-fidelity One Image to 3D Content Generation [[Code]](https://github.com/AILab-CVC/HiFi-123)
- **[DreamDiffusion](https://github.com/bbaaii/DreamDiffusion)**: [ECCV 2024] Generating High-Quality Images from Brain EEG Signals [[Code]](https://github.com/bbaaii/DreamDiffusion)
- **[GVGEN](https://sotamak1r.github.io/gvgen/)**: [ECCV 2024] Text-to-3D Generation with Volumetric Representation [[Code]](https://github.com/SOTAMak1r/GVGEN)
- **[TEXGen](https://cvmi-lab.github.io/TEXGen/)**: [SIGGRAPH Asia 2024] A Generative Diffusion Model for Mesh Textures [[Code]](https://github.com/CVMI-Lab/TEXGen)
- **[Splatter a Video](https://sunyangtian.github.io/spatter_a_video_web/)**: [NeurIPS 2024] Video Gaussian Representation for Versatile Processing [[Code]](https://github.com/SunYangtian/Splatter_A_Video)
- **[UniDream](https://yg256li.github.io/UniDream/)**: [ECCV 2024] Unifying Diffusion Priors for Relightable Text-to-3D Generation
- **[PI3D](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_PI3D_Efficient_Text-to-3D_Generation_with_Pseudo-Image_Diffusion_CVPR_2024_paper.pdf)**: [CVPR 2024] Efficient Text-to-3D Generation with Pseudo-Image Diffusion
- **[DMiT](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07243.pdf)**: [ECCV 2024] Deformable Mipmapped Tri-Plane Representation for Dynamic Scenes
- **[SAMPart3D](https://yhyang-myron.github.io/SAMPart3D-website/)**: Segment Any Part in 3D Objects [[Code]](https://github.com/Pointcept/SAMPart3D)
- **[DreamCraft3D++](https://dreamcraft3dplus.github.io/)**: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model [[Code]](https://github.com/MrTornado24/DreamCraft3D_Plus)

## üìë Research Papers

- **UniDream**: [ECCV 2024] Unifying Diffusion Priors for Relightable Text-to-3D Generation [[Project Page]](https://yg256li.github.io/UniDream/)
- **PI3D**: [CVPR 2024] Efficient Text-to-3D Generation with Pseudo-Image Diffusion [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_PI3D_Efficient_Text-to-3D_Generation_with_Pseudo-Image_Diffusion_CVPR_2024_paper.pdf)
- **DMiT**: [ECCV 2024] Deformable Mipmapped Tri-Plane Representation for Dynamic Scenes [[Paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07243.pdf)

## üí° Features

- State-of-the-art 3D generation capabilities
- Easy-to-use APIs and SDKs
- Flexible integration options
- High-quality output
- Research-backed technology

## üöÄ Getting Started

Visit our individual repositories to get started with specific components of the Tripo ecosystem:
- For Python integration, check out our `tripo-python-sdk`
- For ComfyUI users, explore `ComfyUI-Tripo`
- For server deployment, see `tripo-mcp`

## üìö Documentation

Each project contains detailed documentation in its respective repository. Check the individual repos for:
- Installation guides
- API documentation
- Usage examples
- Best practices

## ü§ù Contributing

We welcome contributions from the community! Please check individual project repositories for contribution guidelines.

## üìÑ Licenses

Our projects are released under various licenses including MIT and Apache-2.0. Please check individual repositories for specific licensing information.

## üì´ Contact

For questions and collaborations, please open issues in the respective project repositories.

---

*Follow us to stay updated with the latest developments in AI-powered 3D generation!*
